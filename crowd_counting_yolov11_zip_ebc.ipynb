{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HeraldoArman/RailGuard/blob/main/crowd_counting_yolov11_zip_ebc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Repo Git"
      ],
      "metadata": {
        "id": "6_zTgJ3NHvuV"
      },
      "id": "6_zTgJ3NHvuV"
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone own repo\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# github_username = 'Delta-K-rist'\n",
        "# github_repo_name = 'kai-ai-model'\n",
        "\n",
        "# github_token = userdata.get('GITHUB_TOKEN_DELTA')\n",
        "\n",
        "# authenticated_github_url = f'https://{github_username}:{github_token}@github.com/{github_username}/{github_repo_name}.git'\n",
        "\n",
        "# !git clone {authenticated_github_url}"
      ],
      "metadata": {
        "id": "FTehAwcTHx0_"
      },
      "id": "FTehAwcTHx0_",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Delta-K-rist/kai-ai-model.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEc-dvmo4KDW",
        "outputId": "21059e83-eaf2-48f2-b286-f318937c4685"
      },
      "id": "aEc-dvmo4KDW",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'kai-ai-model'...\n",
            "remote: Enumerating objects: 67, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 67 (delta 0), reused 0 (delta 0), pack-reused 64 (from 1)\u001b[K\n",
            "Receiving objects: 100% (67/67), 241.16 MiB | 15.44 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Yiming-M/ZIP.git ZIP_Crowd_Counting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEtHShnyEeo_",
        "outputId": "f19cfce3-ebec-49eb-fbb6-9fa75ec3c67d"
      },
      "id": "kEtHShnyEeo_",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ZIP_Crowd_Counting'...\n",
            "remote: Enumerating objects: 126, done.\u001b[K\n",
            "remote: Counting objects: 100% (126/126), done.\u001b[K\n",
            "remote: Compressing objects: 100% (108/108), done.\u001b[K\n",
            "remote: Total 126 (delta 33), reused 96 (delta 17), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (126/126), 2.85 MiB | 2.15 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations"
      ],
      "metadata": {
        "id": "jFgrx6ggsK2R"
      },
      "id": "jFgrx6ggsK2R"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q ultralytics opencv-python-headless"
      ],
      "metadata": {
        "id": "VRFekfdosOUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40bd8852-6e49-463e-eb86-ebf0eedc511b"
      },
      "id": "VRFekfdosOUF",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q ultralytics \"lap>=0.5.12\""
      ],
      "metadata": {
        "id": "XELADI8hfv5R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a03c28a-4b4f-46b8-d0d7-3582e62803b5"
      },
      "id": "XELADI8hfv5R",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m119.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check Versions"
      ],
      "metadata": {
        "id": "q-7wRHPyZ013"
      },
      "id": "q-7wRHPyZ013"
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "import torchvision\n",
        "import timm\n",
        "import numpy\n",
        "import scipy\n",
        "import yaml  # PyYAML library\n",
        "import peft\n",
        "\n",
        "print(\"--- System Information ---\")\n",
        "print(f\"Python Version: {sys.version.split()[0]}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"PyTorch Version: {torch.__version__}\")\n",
        "else:\n",
        "    print(\"CUDA is not available. PyTorch is running on CPU.\")\n",
        "    print(f\"PyTorch Version: {torch.__version__}\")\n",
        "\n",
        "print(\"\\n--- Key Package Versions ---\")\n",
        "print(f\"Torchvision: {torchvision.__version__}\")\n",
        "print(f\"Timm: {timm.__version__}\")\n",
        "print(f\"PEFT: {peft.__version__}\")\n",
        "print(f\"NumPy: {numpy.__version__}\")\n",
        "print(f\"SciPy: {scipy.__version__}\")\n",
        "print(f\"PyYAML: {yaml.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBgDeGXOZrEE",
        "outputId": "f081a0ab-a2e5-46d0-833a-5f8ebaf70332"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- System Information ---\n",
            "Python Version: 3.12.11\n",
            "CUDA Version: 12.6\n",
            "PyTorch Version: 2.8.0+cu126\n",
            "\n",
            "--- Key Package Versions ---\n",
            "Torchvision: 0.23.0+cu126\n",
            "Timm: 1.0.20\n",
            "PEFT: 0.17.1\n",
            "NumPy: 2.0.2\n",
            "SciPy: 1.16.2\n",
            "PyYAML: 6.0.3\n"
          ]
        }
      ],
      "id": "oBgDeGXOZrEE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download Versions"
      ],
      "metadata": {
        "id": "zfBxD_hwZ3UF"
      },
      "id": "zfBxD_hwZ3UF"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Installing the specified list of packages...\")\n",
        "\n",
        "!pip install -q \\\n",
        "    einops \\\n",
        "    pyturbojpeg \\\n",
        "    tensorboardX \\\n",
        "    open_clip_torch\n",
        "\n",
        "print(\"✅ All specified packages have been installed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkwPcIWfY1kH",
        "outputId": "a3ec40e5-9b82-4d48-e31b-220d37993d36"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing the specified list of packages...\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyturbojpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "✅ All specified packages have been installed.\n"
          ]
        }
      ],
      "id": "TkwPcIWfY1kH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Initialization (YOLOv11.m and EBC-ZIP)"
      ],
      "metadata": {
        "id": "TX5rNgr1ssLM"
      },
      "id": "TX5rNgr1ssLM"
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XpocO5oLz8q",
        "outputId": "38a057d2-e1cc-4057-8d04-4249b27c3960"
      },
      "id": "2XpocO5oLz8q",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ZIP_Crowd_Counting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C50k_DwQEyOM",
        "outputId": "48ae3404-2781-4790-e11e-b1f36e80ad60"
      },
      "id": "C50k_DwQEyOM",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ZIP_Crowd_Counting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create the full directory path (your command was perfect for this)\n",
        "!mkdir -p /content/ZIP_Crowd_Counting/checkpoints/sha\n",
        "\n",
        "# 2. Download the file directly into that new directory\n",
        "# The -P flag specifies the output directory\n",
        "!wget https://github.com/Yiming-M/ZIP/releases/download/weights_sha/ebc_s.zip -P /content/ZIP_Crowd_Counting/checkpoints/sha\n",
        "\n",
        "# 3. Unzip the file in its destination directory\n",
        "# The -d flag specifies the destination directory for the unzipped files\n",
        "!unzip /content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s.zip -d /content/ZIP_Crowd_Counting/checkpoints/sha"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMyD6NZL_oLp",
        "outputId": "c5f5cfc6-0fbe-42d2-dec6-9e078cf4b70b"
      },
      "id": "qMyD6NZL_oLp",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-03 21:04:03--  https://github.com/Yiming-M/ZIP/releases/download/weights_sha/ebc_s.zip\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/1009736049/cfa45342-7cfe-4056-8b36-168b9e05fb78?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-10-03T21%3A56%3A05Z&rscd=attachment%3B+filename%3Debc_s.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-10-03T20%3A55%3A33Z&ske=2025-10-03T21%3A56%3A05Z&sks=b&skv=2018-11-09&sig=Wtc7Z7iaKm%2BDevQCBtOXoAKGWE27pAwvybj5dIl4EhA%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1OTUyOTA0MywibmJmIjoxNzU5NTI1NDQzLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.cF7-K6osFKZ--cgvEF30ipleuEuaPk5TAEvcniOZLJ0&response-content-disposition=attachment%3B%20filename%3Debc_s.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-10-03 21:04:03--  https://release-assets.githubusercontent.com/github-production-release-asset/1009736049/cfa45342-7cfe-4056-8b36-168b9e05fb78?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-10-03T21%3A56%3A05Z&rscd=attachment%3B+filename%3Debc_s.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-10-03T20%3A55%3A33Z&ske=2025-10-03T21%3A56%3A05Z&sks=b&skv=2018-11-09&sig=Wtc7Z7iaKm%2BDevQCBtOXoAKGWE27pAwvybj5dIl4EhA%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1OTUyOTA0MywibmJmIjoxNzU5NTI1NDQzLCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.cF7-K6osFKZ--cgvEF30ipleuEuaPk5TAEvcniOZLJ0&response-content-disposition=attachment%3B%20filename%3Debc_s.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 504535392 (481M) [application/octet-stream]\n",
            "Saving to: ‘/content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s.zip’\n",
            "\n",
            "ebc_s.zip           100%[===================>] 481.16M  36.1MB/s    in 13s     \n",
            "\n",
            "2025-10-03 21:04:17 (36.3 MB/s) - ‘/content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s.zip’ saved [504535392/504535392]\n",
            "\n",
            "Archive:  /content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s.zip\n",
            "   creating: /content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s/\n",
            "  inflating: /content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s/best_mae.log  \n",
            "  inflating: /content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s/best_mae.pth  \n",
            "  inflating: /content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s/best_nae.log  \n",
            "  inflating: /content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s/best_nae.pth  \n",
            "  inflating: /content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s/best_rmse.log  \n",
            "  inflating: /content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s/best_rmse.pth  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "if '.' not in sys.path:\n",
        "    sys.path.append('.')\n",
        "\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "from models import get_model\n",
        "\n",
        "def initialize_all_models(zip_checkpoint_path):\n",
        "    \"\"\"\n",
        "    Loads and initializes both the YOLOv8 and ZIP models.\n",
        "    \"\"\"\n",
        "    print(\"🧠 Initializing all AI models...\")\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # --- Load YOLOv8 Model ---\n",
        "    yolo_model = YOLO('yolo11m.pt')\n",
        "    yolo_model.to(device)\n",
        "    print(f\"YOLOv11 model loaded on {device}.\")\n",
        "\n",
        "    # --- Load ZIP Model (using your reference code) ---\n",
        "    print(f\"Loading ZIP model from: {zip_checkpoint_path}\")\n",
        "    zip_model = get_model(zip_checkpoint_path)\n",
        "    zip_model = zip_model.to(device)\n",
        "    zip_model.eval() # Set model to evaluation mode\n",
        "    print(f\"ZIP model loaded on {device}.\")\n",
        "\n",
        "    # Return all models in a dictionary\n",
        "    models = {\n",
        "        'yolo': yolo_model,\n",
        "        'zip': zip_model,\n",
        "        'device': device # Store the device for later use\n",
        "    }\n",
        "    return models"
      ],
      "metadata": {
        "id": "7tZ1ZoZ7-Qb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "713f68d0-cc84-4525-b5f2-d13948798bd7"
      },
      "id": "7tZ1ZoZ7-Qb2",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ZIP Setup"
      ],
      "metadata": {
        "id": "Wr-4KW-pSUo-"
      },
      "id": "Wr-4KW-pSUo-"
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision.transforms import ToTensor, Normalize, Compose\n",
        "import cv2\n",
        "\n",
        "# Define the transformation for the ZIP model once\n",
        "zip_transform = Compose([\n",
        "    ToTensor(),\n",
        "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "def get_count_from_density_map(frame, zip_model, device):\n",
        "    \"\"\"\n",
        "    Takes a single video frame, processes it with the ZIP model,\n",
        "    and returns both the estimated person count and the density map.\n",
        "    \"\"\"\n",
        "    # ... (Image conversion and transformation logic is the same)\n",
        "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    image_pil = Image.fromarray(image_rgb)\n",
        "    image_tensor = zip_transform(image_pil)\n",
        "    image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predicted_density = zip_model(image_tensor)\n",
        "\n",
        "    predicted_count = predicted_density.sum().item()\n",
        "\n",
        "    # --- MODIFIED: Return the density map as well ---\n",
        "    return predicted_count, predicted_density"
      ],
      "metadata": {
        "id": "uaHgftP-B0fF"
      },
      "id": "uaHgftP-B0fF",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define path to your ZIP model checkpoint ---\n",
        "# MAKE SURE YOU HAVE UPLOADED THIS FILE\n",
        "ZIP_CHECKPOINT = \"/content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s/best_mae.pth\"\n",
        "\n",
        "# 1. Load both models\n",
        "all_models = initialize_all_models(ZIP_CHECKPOINT)\n",
        "\n",
        "# 2. Create a dummy frame to test the density function\n",
        "# (A black image of size 640x480)\n",
        "import numpy as np\n",
        "dummy_frame = np.zeros((480, 640, 3), dtype=np.uint8)\n",
        "\n",
        "# 3. Run the test\n",
        "if all_models.get('zip'):\n",
        "    # --- MODIFIED: Unpack the tuple into two variables ---\n",
        "    test_count, test_density_map = get_count_from_density_map(dummy_frame, all_models['zip'], all_models['device'])\n",
        "\n",
        "    print(f\"\\n✅ Test successful!\")\n",
        "    # This print statement will now work correctly\n",
        "    print(f\"Count from dummy frame using ZIP model: {test_count:.2f}\")\n",
        "else:\n",
        "    print(\"\\n❌ Test failed. ZIP model not loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312,
          "referenced_widgets": [
            "fad5c2dfbefe4b73931cff1e54409c66",
            "09cee5b8957c4d539847faa3f2786eed",
            "d6c36a32dd7945918c62ee4443040b26",
            "da5679da1d5d4581b9a588b9dbe26172",
            "e8b7d23a718e4cf8962e735f76092a11",
            "5ba3006ccec14f31b9ed304c7070cbbf",
            "89b131d5772e4d73afb485f5f583f145",
            "d7121790a20449a6b101835e158d22de",
            "80aa14d9f8b24239a8ba88dc5bda1ec7",
            "f0fd6bdf80d04938bc1c9a335b9063b9",
            "902bb413d3ea4833b25d7f14e216fde2"
          ]
        },
        "id": "8JyF3UbqMzoV",
        "outputId": "1b56d64a-71b2-4127-cb78-ab4c197f37b9"
      },
      "id": "8JyF3UbqMzoV",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 Initializing all AI models...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m.pt to 'yolo11m.pt': 100% ━━━━━━━━━━━━ 38.8MB 200.4MB/s 0.2s\n",
            "YOLOv11 model loaded on cuda.\n",
            "Loading ZIP model from: /content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s/best_mae.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "open_clip_model.safetensors:   0%|          | 0.00/340M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fad5c2dfbefe4b73931cff1e54409c66"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP model loaded on cuda.\n",
            "\n",
            "✅ Test successful!\n",
            "Count from dummy frame using ZIP model: 0.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video_with_fusion(video_path, models, output_dir, polygon_coords):\n",
        "    \"\"\"\n",
        "    Final version:\n",
        "    - Calculation: Spatial Fusion (YOLO outside + ZIP inside).\n",
        "    - Visualization: Uses the automatic, rich yolo.plot() output blended\n",
        "      with the heatmap, avoiding all manual box drawing.\n",
        "    \"\"\"\n",
        "    print(f\"📹 Starting Spatial Fusion processing for: {video_path}\")\n",
        "\n",
        "    # --- Video Setup (same as before) ---\n",
        "    cap = cv2.VideoCapture(video_path); frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)); frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)); fps = int(cap.get(cv2.CAP_PROP_FPS)); cap.release()\n",
        "    output_video_path = os.path.join(output_dir, \"output_video_spatial_fusion.mp4\"); fourcc = cv2.VideoWriter_fourcc(*'mp4v'); writer = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    yolo_model, zip_model, device = models['yolo'], models['zip'], models['device']\n",
        "    yolo_results_generator = yolo_model.track(source=video_path, persist=True, tracker=\"bytetrack.yaml\", classes=0, stream=True, verbose=False)\n",
        "\n",
        "    roi_polygon = np.array(polygon_coords, dtype=np.int32)\n",
        "    all_frames_data = []; inference_times = []\n",
        "\n",
        "    for frame_results in yolo_results_generator:\n",
        "        original_frame = frame_results.orig_img\n",
        "        inference_times.append(frame_results.speed['inference'])\n",
        "        yolo_boxes = frame_results.boxes.data.cpu().numpy()\n",
        "        yolo_count = len(yolo_boxes)\n",
        "\n",
        "        final_annotated_frame, final_count, avg_confidence, confidences = (None, 0, 0.0, [])\n",
        "\n",
        "        if yolo_count < 10:\n",
        "            # --- Case 1: VERY SPARSE (Unchanged) ---\n",
        "            final_count = yolo_count\n",
        "            final_annotated_frame = frame_results.plot(line_width=1, font_size=0.2)\n",
        "            cv2.polylines(final_annotated_frame, [roi_polygon], isClosed=True, color=(0, 255, 0), thickness=2)\n",
        "            cv2.putText(final_annotated_frame, \"MODE: Pure YOLO (Sparse)\", (30, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
        "            if frame_results.boxes.conf is not None:\n",
        "                confidences = frame_results.boxes.conf.tolist()\n",
        "                if confidences: avg_confidence = sum(confidences) / len(confidences)\n",
        "\n",
        "        else:\n",
        "            # --- Case 2: MEDIUM/DENSE - Spatial Fusion Calculation ---\n",
        "            zip_count_full, density_map = get_count_from_density_map(original_frame, zip_model, device)\n",
        "            outside_yolo_count = 0\n",
        "            for box in yolo_boxes:\n",
        "                box_center = (int((box[0] + box[2]) / 2), int((box[1] + box[3]) / 2))\n",
        "                if cv2.pointPolygonTest(roi_polygon, box_center, False) < 0:\n",
        "                    outside_yolo_count += 1\n",
        "            density_map_cpu = density_map.squeeze().cpu().numpy()\n",
        "            scale_x, scale_y = density_map_cpu.shape[1] / frame_width, density_map_cpu.shape[0] / frame_height\n",
        "            scaled_polygon = (roi_polygon * [scale_x, scale_y]).astype(np.int32)\n",
        "            mask = np.zeros(density_map_cpu.shape, dtype=np.uint8); cv2.fillPoly(mask, [scaled_polygon], 255)\n",
        "            inside_zip_count = (density_map_cpu * (mask / 255.0)).sum()\n",
        "            final_count = outside_yolo_count + round(inside_zip_count)\n",
        "\n",
        "            # --- MODIFIED: Reverted to the .plot() visualization ---\n",
        "            # 1. Get the default, rich annotation from YOLO's .plot()\n",
        "            yolo_annotated_frame = frame_results.plot(line_width=1, font_size=0.5)\n",
        "\n",
        "            # 2. Generate the full-frame heatmap\n",
        "            norm_map = cv2.normalize(density_map_cpu, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
        "            heatmap = cv2.applyColorMap(norm_map, cv2.COLORMAP_JET)\n",
        "            heatmap = cv2.resize(heatmap, (frame_width, frame_height))\n",
        "\n",
        "            # 3. Blend the full YOLO annotations with the heatmap\n",
        "            blended_frame = cv2.addWeighted(yolo_annotated_frame, 0.6, heatmap, 0.4, 0)\n",
        "\n",
        "            # 4. Draw the polygon on top\n",
        "            cv2.polylines(blended_frame, [roi_polygon], isClosed=True, color=(0, 255, 0), thickness=2)\n",
        "\n",
        "            final_annotated_frame = blended_frame\n",
        "            # We no longer need the manual drawing text, just the final mode\n",
        "            cv2.putText(final_annotated_frame, f\"MODE: Spatial Fusion\", (30, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "            cv2.putText(final_annotated_frame, f\"  - Outside (YOLO): {outside_yolo_count}\", (30, 110), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "            cv2.putText(final_annotated_frame, f\"  - Inside (ZIP-EBC): {round(inside_zip_count)}\", (30, 140), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "\n",
        "            avg_confidence = np.mean(yolo_boxes[:, 5]) if len(yolo_boxes) > 0 else 0\n",
        "            confidences = yolo_boxes[:, 5].tolist()\n",
        "\n",
        "        # Add the final count text to every frame\n",
        "        cv2.putText(final_annotated_frame, f\"Final Count: {final_count}\", (30, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "        writer.write(final_annotated_frame)\n",
        "\n",
        "        all_frames_data.append({ \"count\": final_count, \"avg_confidence\": avg_confidence, \"annotated_frame\": final_annotated_frame, \"original_frame\": original_frame, \"confidences\": confidences })\n",
        "\n",
        "    # ... (Phase 2 analysis remains unchanged) ...\n",
        "    writer.release(); print(\"✅ Data collection complete.\")\n",
        "    print(\"🧠 Starting Phase 2: Finding the best frame...\");\n",
        "    if not all_frames_data: return [], None, None, [], []\n",
        "    absolute_max_count = max(frame['count'] for frame in all_frames_data)\n",
        "    candidate_frames = [f for f in all_frames_data if f['count'] >= absolute_max_count * 0.95]\n",
        "    best_frame = max(candidate_frames, key=lambda x: x['avg_confidence']) if candidate_frames else max(all_frames_data, key=lambda x: x['count'])\n",
        "    print(f\"🏆 Best frame selected: Count={best_frame['count']}, Confidence={best_frame['avg_confidence']:.2f}\")\n",
        "    final_annotated_snapshot, final_original_snapshot = best_frame['annotated_frame'].copy(), best_frame['original_frame'].copy()\n",
        "    final_confidences_at_max = best_frame['confidences']\n",
        "    frame_by_frame_counts = [frame['count'] for frame in all_frames_data]\n",
        "\n",
        "    return frame_by_frame_counts, final_annotated_snapshot, final_original_snapshot, inference_times, final_confidences_at_max"
      ],
      "metadata": {
        "id": "1QkI0WcENmbE"
      },
      "id": "1QkI0WcENmbE",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "GAuTZ9X8YUWn"
      },
      "id": "GAuTZ9X8YUWn"
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "def preprocess_video(input_path, output_path, target_fps=15, target_width=1280):\n",
        "    \"\"\"\n",
        "    Creates an optimized version of a video for faster model processing.\n",
        "\n",
        "    Args:\n",
        "        input_path (str): Path to the original video file.\n",
        "        output_path (str): Path to save the new, preprocessed video.\n",
        "        target_fps (int): The desired frames per second.\n",
        "        target_width (int): The desired frame width. Height is scaled automatically.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the newly created video file.\n",
        "    \"\"\"\n",
        "    print(f\"🔧 Starting preprocessing for {input_path}...\")\n",
        "\n",
        "    # 1. Open the original video\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video.\")\n",
        "        return None\n",
        "\n",
        "    # 2. Get original video properties\n",
        "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # 3. Calculate resampling and resizing parameters\n",
        "    skip_interval = max(1, round(original_fps / target_fps))\n",
        "    aspect_ratio = original_height / original_width\n",
        "    target_height = int(target_width * aspect_ratio)\n",
        "\n",
        "    print(f\"Original: {original_width}x{original_height} @ {original_fps:.2f} FPS\")\n",
        "    print(f\"Target:   {target_width}x{target_height} @ {target_fps} FPS (keeping 1 in every {skip_interval} frames)\")\n",
        "\n",
        "    # 4. Initialize the Video Writer for the new video\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    writer = cv2.VideoWriter(output_path, fourcc, target_fps, (target_width, target_height))\n",
        "\n",
        "    # 5. Loop, Resample, Resize, and Write\n",
        "    frame_number = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break # End of video\n",
        "\n",
        "        # Only process a frame if it's at the correct interval\n",
        "        if frame_number % skip_interval == 0:\n",
        "            # Resize the frame\n",
        "            resized_frame = cv2.resize(frame, (target_width, target_height))\n",
        "            # Write the resized frame to the new video\n",
        "            writer.write(resized_frame)\n",
        "\n",
        "        frame_number += 1\n",
        "\n",
        "    # 6. Finalize and clean up\n",
        "    cap.release()\n",
        "    writer.release()\n",
        "\n",
        "    print(f\"✅ Preprocessing complete. Optimized video saved to: {output_path}\")\n",
        "    return output_path"
      ],
      "metadata": {
        "id": "2pBx87PIYWP0"
      },
      "id": "2pBx87PIYWP0",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save"
      ],
      "metadata": {
        "id": "2RUugP0_Svex"
      },
      "id": "2RUugP0_Svex"
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# STEP 2.2 (UPDATED): THE SAVE RESULTS FUNCTION\n",
        "# -----------------------------------------------------------------------------\n",
        "import os\n",
        "import json\n",
        "import cv2\n",
        "\n",
        "def save_analysis_summary(output_dir, counts, annotated_snapshot, original_snapshot, inference_times, video_basename, confidences_at_max):\n",
        "    \"\"\"\n",
        "    Calculates final metrics and saves the output files, including both snapshot versions.\n",
        "    \"\"\"\n",
        "    print(f\"💾 Saving analysis to: {output_dir}\")\n",
        "\n",
        "    if video_basename == 'd_1':\n",
        "        gerbong_id = 'gerbong_3'\n",
        "    elif video_basename == 'm_1':\n",
        "        gerbong_id = 'gerbong_2'\n",
        "    elif video_basename == 's_2':\n",
        "        gerbong_id = 'gerbong_1'\n",
        "    else:\n",
        "        gerbong_id = 'gerbong_unknown' # Default case\n",
        "\n",
        "    avg_confidence = 0\n",
        "    if confidences_at_max:\n",
        "        avg_confidence = sum(confidences_at_max) / len(confidences_at_max)\n",
        "\n",
        "    # ... (all the calculation logic remains the same) ...\n",
        "    human_count = int(max(counts)) if counts else 0\n",
        "    MEDIUM_THRESHOLD = 13\n",
        "    DANGEROUS_THRESHOLD = 27\n",
        "    crowdness_level = \"Low Density\"\n",
        "    if human_count >= DANGEROUS_THRESHOLD:\n",
        "        crowdness_level = \"High Density\"\n",
        "    elif human_count >= MEDIUM_THRESHOLD:\n",
        "        crowdness_level = \"Medium Density\"\n",
        "    total_inference_seconds = sum(inference_times) / 1000.0\n",
        "    avg_inference_ms = sum(inference_times) / len(inference_times) if inference_times else 0\n",
        "\n",
        "    # --- MODIFIED: Save both snapshot images with new names ---\n",
        "    if annotated_snapshot is not None:\n",
        "        # text = f\"Human Count: {human_count} ({crowdness_level})\"\n",
        "        # cv2.putText(annotated_snapshot, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "        snapshot_path = os.path.join(output_dir, 'snapshot_annotated.jpg')\n",
        "        cv2.imwrite(snapshot_path, annotated_snapshot)\n",
        "        print(f\"📸 Annotated snapshot saved to {snapshot_path}\")\n",
        "\n",
        "    if original_snapshot is not None:\n",
        "        snapshot_path_orig = os.path.join(output_dir, 'snapshot_original.jpg')\n",
        "        cv2.imwrite(snapshot_path_orig, original_snapshot)\n",
        "        print(f\"📸 Original snapshot saved to {snapshot_path_orig}\")\n",
        "\n",
        "    # ... (JSON saving logic remains the same) ...\n",
        "    results_data = {\n",
        "        'gerbong_id': gerbong_id,\n",
        "        'max_human_count': human_count,\n",
        "        'confidence_score': round(avg_confidence, 2),\n",
        "        'crowdness_level': crowdness_level,\n",
        "        'performance': {\n",
        "            'total_inference_seconds': round(total_inference_seconds, 2),\n",
        "            'average_inference_ms': round(avg_inference_ms, 2),\n",
        "            'average_fps': round(1000 / avg_inference_ms, 1) if avg_inference_ms > 0 else 'inf'\n",
        "        },\n",
        "        'frame_count_data': counts\n",
        "    }\n",
        "    json_path = os.path.join(output_dir, 'results.json')\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(results_data, f, indent=4)\n",
        "    print(f\"📄 JSON summary saved to {json_path}\")"
      ],
      "metadata": {
        "id": "tSYHBpx9vF0o"
      },
      "id": "tSYHBpx9vF0o",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "e9PJixYLTHis"
      },
      "id": "e9PJixYLTHis"
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# FINAL MAIN EXECUTION BLOCK - BATCH PROCESSING WITH AI FUSION\n",
        "# -----------------------------------------------------------------------------\n",
        "import os\n",
        "import glob\n",
        "from IPython.display import display, Video\n",
        "import json\n",
        "\n",
        "# --- 1. Define Base Paths ---\n",
        "INPUT_FOLDER = '/content/kai-ai-model/YOLO/dataset/test/demo'\n",
        "BASE_OUTPUT_FOLDER = '/content/kai-ai-model/YOLO/dataset/test result'\n",
        "ZIP_CHECKPOINT = \"/content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s/best_mae.pth\"\n",
        "\n",
        "\n",
        "# The polygon coordinates you provided\n",
        "ROI_POLYGON = [[371, 247], [868, 250], [837, 443], [391, 433]]\n",
        "os.makedirs(BASE_OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "# --- 2. Find all video files ---\n",
        "video_files = sorted(list(set(\n",
        "    glob.glob(os.path.join(INPUT_FOLDER, '*.mp4')) + glob.glob(os.path.join(INPUT_FOLDER, '*.MP4')) +\n",
        "    glob.glob(os.path.join(INPUT_FOLDER, '*.mov')) + glob.glob(os.path.join(INPUT_FOLDER, '*.MOV'))\n",
        ")))\n",
        "print(f\"Found {len(video_files)} videos to process.\")\n",
        "\n",
        "# --- 3. Initialize all models ONCE ---\n",
        "print(\"\\n--- INITIALIZING ALL MODELS (ONCE) ---\")\n",
        "all_models = initialize_all_models(ZIP_CHECKPOINT)\n",
        "\n",
        "# --- 4. Loop through each video and run the full pipeline ---\n",
        "for original_video_path in video_files:\n",
        "    print(f\"\\n{'='*50}\\n🎬 PROCESSING VIDEO: {os.path.basename(original_video_path)}\\n{'='*50}\")\n",
        "\n",
        "    video_basename = os.path.splitext(os.path.basename(original_video_path))[0]\n",
        "    final_output_dir = os.path.join(BASE_OUTPUT_FOLDER, video_basename)\n",
        "    os.makedirs(final_output_dir, exist_ok=True)\n",
        "\n",
        "    preprocessed_video_path = os.path.join(final_output_dir, 'preprocessed.mp4')\n",
        "    actual_video_to_process = preprocess_video(original_video_path, preprocessed_video_path)\n",
        "\n",
        "    if actual_video_to_process:\n",
        "        counts, annotated_snap, original_snap, times, confs = process_video_with_fusion(\n",
        "            actual_video_to_process,\n",
        "            all_models,\n",
        "            final_output_dir,\n",
        "            polygon_coords=ROI_POLYGON # Pass the polygon to the function\n",
        "        )\n",
        "\n",
        "        save_analysis_summary(\n",
        "            final_output_dir, counts, annotated_snap,\n",
        "            original_snap, times, video_basename, confs\n",
        "        )\n",
        "        print(f\"✅ SUCCESSFULLY PROCESSED: {os.path.basename(original_video_path)}\")\n",
        "    else:\n",
        "        print(f\"❌ FAILED to preprocess: {os.path.basename(original_video_path)}\")\n",
        "\n",
        "print(f\"\\n\\n{'='*50}\\n🎉 BATCH PROCESSING COMPLETE! 🎉\\n{'='*50}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEa65UvrNqo9",
        "outputId": "06d1293f-f9fa-49a6-8a95-9c357cbd0c48"
      },
      "id": "aEa65UvrNqo9",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3 videos to process.\n",
            "\n",
            "--- INITIALIZING ALL MODELS (ONCE) ---\n",
            "🧠 Initializing all AI models...\n",
            "YOLOv11 model loaded on cuda.\n",
            "Loading ZIP model from: /content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s/best_mae.pth\n",
            "ZIP model loaded on cuda.\n",
            "\n",
            "==================================================\n",
            "🎬 PROCESSING VIDEO: d_1.mp4\n",
            "==================================================\n",
            "🔧 Starting preprocessing for /content/kai-ai-model/YOLO/dataset/test/demo/d_1.mp4...\n",
            "Original: 1280x720 @ 30.00 FPS\n",
            "Target:   1280x720 @ 15 FPS (keeping 1 in every 2 frames)\n",
            "✅ Preprocessing complete. Optimized video saved to: /content/kai-ai-model/YOLO/dataset/test result/d_1/preprocessed.mp4\n",
            "📹 Starting Spatial Fusion processing for: /content/kai-ai-model/YOLO/dataset/test result/d_1/preprocessed.mp4\n",
            "✅ Data collection complete.\n",
            "🧠 Starting Phase 2: Finding the best frame...\n",
            "🏆 Best frame selected: Count=27, Confidence=0.61\n",
            "💾 Saving analysis to: /content/kai-ai-model/YOLO/dataset/test result/d_1\n",
            "📸 Annotated snapshot saved to /content/kai-ai-model/YOLO/dataset/test result/d_1/snapshot_annotated.jpg\n",
            "📸 Original snapshot saved to /content/kai-ai-model/YOLO/dataset/test result/d_1/snapshot_original.jpg\n",
            "📄 JSON summary saved to /content/kai-ai-model/YOLO/dataset/test result/d_1/results.json\n",
            "✅ SUCCESSFULLY PROCESSED: d_1.mp4\n",
            "\n",
            "==================================================\n",
            "🎬 PROCESSING VIDEO: m_1.MOV\n",
            "==================================================\n",
            "🔧 Starting preprocessing for /content/kai-ai-model/YOLO/dataset/test/demo/m_1.MOV...\n",
            "Original: 1920x1080 @ 29.97 FPS\n",
            "Target:   1280x720 @ 15 FPS (keeping 1 in every 2 frames)\n",
            "✅ Preprocessing complete. Optimized video saved to: /content/kai-ai-model/YOLO/dataset/test result/m_1/preprocessed.mp4\n",
            "📹 Starting Spatial Fusion processing for: /content/kai-ai-model/YOLO/dataset/test result/m_1/preprocessed.mp4\n",
            "✅ Data collection complete.\n",
            "🧠 Starting Phase 2: Finding the best frame...\n",
            "🏆 Best frame selected: Count=24, Confidence=0.54\n",
            "💾 Saving analysis to: /content/kai-ai-model/YOLO/dataset/test result/m_1\n",
            "📸 Annotated snapshot saved to /content/kai-ai-model/YOLO/dataset/test result/m_1/snapshot_annotated.jpg\n",
            "📸 Original snapshot saved to /content/kai-ai-model/YOLO/dataset/test result/m_1/snapshot_original.jpg\n",
            "📄 JSON summary saved to /content/kai-ai-model/YOLO/dataset/test result/m_1/results.json\n",
            "✅ SUCCESSFULLY PROCESSED: m_1.MOV\n",
            "\n",
            "==================================================\n",
            "🎬 PROCESSING VIDEO: s_2.MOV\n",
            "==================================================\n",
            "🔧 Starting preprocessing for /content/kai-ai-model/YOLO/dataset/test/demo/s_2.MOV...\n",
            "Original: 1920x1080 @ 29.97 FPS\n",
            "Target:   1280x720 @ 15 FPS (keeping 1 in every 2 frames)\n",
            "✅ Preprocessing complete. Optimized video saved to: /content/kai-ai-model/YOLO/dataset/test result/s_2/preprocessed.mp4\n",
            "📹 Starting Spatial Fusion processing for: /content/kai-ai-model/YOLO/dataset/test result/s_2/preprocessed.mp4\n",
            "✅ Data collection complete.\n",
            "🧠 Starting Phase 2: Finding the best frame...\n",
            "🏆 Best frame selected: Count=14, Confidence=0.52\n",
            "💾 Saving analysis to: /content/kai-ai-model/YOLO/dataset/test result/s_2\n",
            "📸 Annotated snapshot saved to /content/kai-ai-model/YOLO/dataset/test result/s_2/snapshot_annotated.jpg\n",
            "📸 Original snapshot saved to /content/kai-ai-model/YOLO/dataset/test result/s_2/snapshot_original.jpg\n",
            "📄 JSON summary saved to /content/kai-ai-model/YOLO/dataset/test result/s_2/results.json\n",
            "✅ SUCCESSFULLY PROCESSED: s_2.MOV\n",
            "\n",
            "\n",
            "==================================================\n",
            "🎉 BATCH PROCESSING COMPLETE! 🎉\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The '-r' flag means 'recursive' to include all subfolders\n",
        "# !zip -r /content/results.zip '/content/kai-ai-model/YOLO/dataset/test result'"
      ],
      "metadata": {
        "id": "zaqx5OWpo5iL"
      },
      "id": "zaqx5OWpo5iL",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn pyngrok nest_asyncio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPd6F6yK3O-S",
        "outputId": "9979d337-c2b9-497f-e10b-53c25dccc31d"
      },
      "id": "TPd6F6yK3O-S",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.118.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.37.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.4.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.48.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fastapi) (2.11.9)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (4.15.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.3.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.49.0,>=0.40.0->fastapi) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.49.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.49.0,>=0.40.0->fastapi) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "url = \"https://rail-guard-pwa.vercel.app/api/webhook\"\n",
        "\n",
        "# 1) GET (harusnya bukan 404, idealnya 405/400 kalau route ada)\n",
        "r = requests.get(url, timeout=15)\n",
        "print(\"GET status:\", r.status_code, r.text[:200])\n",
        "\n",
        "# 2) OPTIONS preflight (kalau ada middleware aneh, kadang 404)\n",
        "r = requests.options(url, timeout=15)\n",
        "print(\"OPTIONS status:\", r.status_code)\n",
        "\n",
        "# 3) POST kosong utk lihat status (harusnya 400/415/500, tapi bukan 404)\n",
        "r = requests.post(url, json={}, timeout=15)\n",
        "print(\"POST status:\", r.status_code, r.text[:200])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcQvICdU8kRk",
        "outputId": "09676497-cc3f-4333-fa67-9a5bef3121d8"
      },
      "id": "vcQvICdU8kRk",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GET status: 405 \n",
            "OPTIONS status: 204\n",
            "POST status: 400 {\"success\":false,\"error\":\"Missing required fields\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, base64, requests, math, random\n",
        "\n",
        "WEBHOOK_URL = \"https://rail-guard-pwa.vercel.app/api/webhook\"  # punyamu\n",
        "\n",
        "def b64_image(path: str):\n",
        "    with open(path, \"rb\") as f:\n",
        "        return base64.b64encode(f.read()).decode(\"utf-8\")  # raw base64 (backendmu sudah handle prefix)\n",
        "\n",
        "def safe_fps(v):\n",
        "    # results.json kamu kadang menyimpan \"inf\". Backend TypeScript menulis average_fps: number,\n",
        "    # jadi konversi \"inf\" jadi float besar atau 0 (pilih salah satu).\n",
        "    if isinstance(v, str):\n",
        "        if v.lower() == \"inf\":\n",
        "            return 0.0  # atau 1e9\n",
        "        try:\n",
        "            return float(v)\n",
        "        except:\n",
        "            return 0.0\n",
        "    return float(v)\n",
        "\n",
        "def post_to_webhook_from_results(result_dir: str, send_image: bool = True):\n",
        "    # baca ringkasan yang kamu simpan\n",
        "    jpath = os.path.join(result_dir, \"results.json\")\n",
        "    with open(jpath, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # siapkan payload sesuai interface WebhookPayload di backend\n",
        "    # Kalau DB kamu memang NanoID, map dulu:\n",
        "    GERBONG_MAP = {\n",
        "      \"gerbong_1\": \"HNXqlqqJQHdzxy1tiNBpS\",\n",
        "      \"gerbong_2\": \"HcbmJasaKOMc93W7znqPy\",\n",
        "      \"gerbong_3\": \"MFbltcSi5tGlFKgeumHWM\",\n",
        "      \"gerbong_4\": \"OKSG6vtcfxD1Y5bLpL5FH\",\n",
        "      }\n",
        "    raw_id = data.get(\"gerbong_id\", \"gerbong_unknown\")\n",
        "    gerbong_id = GERBONG_MAP.get(raw_id, raw_id)\n",
        "    payload = {\n",
        "        \"gerbong_id\": gerbong_id,\n",
        "        \"max_human_count\": int(data.get(\"max_human_count\", 0)),\n",
        "        \"confidence_score\": float(data.get(\"confidence_score\", 0.0)),\n",
        "        \"crowdness_level\": data.get(\"crowdness_level\", \"Low Density\"),  # \"Low Density\" | \"Medium Density\" | \"High Density\"\n",
        "        \"performance\": {\n",
        "            \"total_inference_seconds\": float(data[\"performance\"].get(\"total_inference_seconds\", 0.0)),\n",
        "            \"average_inference_ms\": float(data[\"performance\"].get(\"average_inference_ms\", 0.0)),\n",
        "            \"average_fps\": safe_fps(data[\"performance\"].get(\"average_fps\", 0.0)),\n",
        "        },\n",
        "        # kamu boleh kirim \"image\" (base64) ATAU \"image_url\"\n",
        "        # \"image_url\": \"https://contoh.com/snapshot.jpg\"\n",
        "    }\n",
        "\n",
        "    if send_image:\n",
        "        annotated = os.path.join(result_dir, \"snapshot_annotated.jpg\")\n",
        "        if os.path.exists(annotated):\n",
        "            with open(annotated, \"rb\") as f:\n",
        "                payload[\"image\"] = base64.b64encode(f.read()).decode(\"utf-8\")\n",
        "\n",
        "    headers = {\"Content-Type\": \"application/json\", \"Accept\": \"*/*\"}\n",
        "    r = requests.post(\"https://rail-guard-pwa.vercel.app/api/webhook\", headers=headers, json=payload, timeout=60)\n",
        "    print(\"Status:\", r.status_code)\n",
        "    print(\"Body  :\", r.text[:500])\n",
        "    r.raise_for_status()\n",
        "    return r.json()\n"
      ],
      "metadata": {
        "id": "qI8y3IEm3f0g"
      },
      "id": "qI8y3IEm3f0g",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vfuL65u3AK9Q"
      },
      "id": "vfuL65u3AK9Q"
    },
    {
      "cell_type": "code",
      "source": [
        "# ...setelah save_analysis_summary(...)\n",
        "save_analysis_summary(\n",
        "    final_output_dir, counts, annotated_snap,\n",
        "    original_snap, times, video_basename, confs\n",
        ")\n",
        "\n",
        "# setelah save_analysis_summary(...)\n",
        "post_to_webhook_from_results(final_output_dir, send_image=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Umimu5Qe70mE",
        "outputId": "82982baa-294d-4d69-ab63-4c2763657440"
      },
      "id": "Umimu5Qe70mE",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "💾 Saving analysis to: /content/kai-ai-model/YOLO/dataset/test result/s_2\n",
            "📸 Annotated snapshot saved to /content/kai-ai-model/YOLO/dataset/test result/s_2/snapshot_annotated.jpg\n",
            "📸 Original snapshot saved to /content/kai-ai-model/YOLO/dataset/test result/s_2/snapshot_original.jpg\n",
            "📄 JSON summary saved to /content/kai-ai-model/YOLO/dataset/test result/s_2/results.json\n",
            "Status: 200\n",
            "Body  : {\"success\":true,\"message\":\"Webhook processed successfully\",\"data\":{\"gerbong_updated\":true,\"case_created\":false,\"occupancy_level\":\"sedang\",\"ai_analysis\":\"Gambar menunjukkan sekitar 14 penumpang di dalam gerbong kereta. Tingkat kepadatan penumpang tergolong sedang, dengan beberapa kursi masih tersedia dan beberapa penumpang berdiri. Kondisi umum gerbong tampak tertib dan tidak ada anomali yang mencolok.\"}}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'success': True,\n",
              " 'message': 'Webhook processed successfully',\n",
              " 'data': {'gerbong_updated': True,\n",
              "  'case_created': False,\n",
              "  'occupancy_level': 'sedang',\n",
              "  'ai_analysis': 'Gambar menunjukkan sekitar 14 penumpang di dalam gerbong kereta. Tingkat kepadatan penumpang tergolong sedang, dengan beberapa kursi masih tersedia dan beberapa penumpang berdiri. Kondisi umum gerbong tampak tertib dan tidak ada anomali yang mencolok.'}}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2yduLhdNq4MaA9CHDQNkRMiDXOV_ZDLKVot8djT2FNHa2ZNE"
      ],
      "metadata": {
        "id": "j2Lauw8hGDbX",
        "outputId": "cae5cc8b-598a-40d6-fec6-4c79819271f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "j2Lauw8hGDbX",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Jalankan FastAPI di Notebook ---\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from fastapi import FastAPI\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Patch asyncio agar uvicorn bisa jalan di colab/notebook\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Buat app FastAPI\n",
        "app = FastAPI()\n",
        "\n",
        "BASE_RESULT_DIR = \"/content/kai-ai-model/YOLO/dataset/test result\"\n",
        "\n",
        "@app.get(\"/\")\n",
        "def root():\n",
        "    return {\"message\": \"FastAPI demo is running. Use /snapshot/{video_name}\"}\n",
        "\n",
        "@app.get(\"/snapshot/{video_name}\")\n",
        "def get_snapshot(video_name: str):\n",
        "    \"\"\"\n",
        "    video_name = nama folder hasil video (misal: d_1, m_1, s_2)\n",
        "    akan mengembalikan snapshot_annotated.jpg kalau ada\n",
        "    \"\"\"\n",
        "    path = os.path.join(BASE_RESULT_DIR, video_name, \"snapshot_annotated.jpg\")\n",
        "    if not os.path.exists(path):\n",
        "        return {\"error\": f\"Snapshot not found for {video_name}\"}\n",
        "    return FileResponse(path, media_type=\"image/jpeg\")\n",
        "\n",
        "# Konfigurasi server\n",
        "config = uvicorn.Config(app=app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "server = uvicorn.Server(config)\n",
        "\n",
        "# Buka tunnel ngrok\n",
        "public_url = ngrok.connect(8000)\n",
        "print(\"Public URL:\", public_url)\n",
        "\n",
        "# Jalankan server (blocking, tapi aman di notebook)\n",
        "await server.serve()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix2rWle4EWOg",
        "outputId": "297a0353-bd13-486d-9936-d571ad74cd27"
      },
      "id": "ix2rWle4EWOg",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [255]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://80f5769d6e4c.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "INFO:     2404:c0:2520::23f:5d4f:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     2404:c0:2520::23f:5d4f:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO:     2404:c0:2520::23f:5d4f:0 - \"GET /snapshot/gerbong_1 HTTP/1.1\" 200 OK\n",
            "INFO:     2404:c0:2520::23f:5d4f:0 - \"GET /snapshot/ HTTP/1.1\" 404 Not Found\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [255]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "q-7wRHPyZ013"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fad5c2dfbefe4b73931cff1e54409c66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_09cee5b8957c4d539847faa3f2786eed",
              "IPY_MODEL_d6c36a32dd7945918c62ee4443040b26",
              "IPY_MODEL_da5679da1d5d4581b9a588b9dbe26172"
            ],
            "layout": "IPY_MODEL_e8b7d23a718e4cf8962e735f76092a11"
          }
        },
        "09cee5b8957c4d539847faa3f2786eed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ba3006ccec14f31b9ed304c7070cbbf",
            "placeholder": "​",
            "style": "IPY_MODEL_89b131d5772e4d73afb485f5f583f145",
            "value": "open_clip_model.safetensors: 100%"
          }
        },
        "d6c36a32dd7945918c62ee4443040b26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7121790a20449a6b101835e158d22de",
            "max": 340443476,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_80aa14d9f8b24239a8ba88dc5bda1ec7",
            "value": 340443476
          }
        },
        "da5679da1d5d4581b9a588b9dbe26172": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0fd6bdf80d04938bc1c9a335b9063b9",
            "placeholder": "​",
            "style": "IPY_MODEL_902bb413d3ea4833b25d7f14e216fde2",
            "value": " 340M/340M [00:12&lt;00:00, 26.4MB/s]"
          }
        },
        "e8b7d23a718e4cf8962e735f76092a11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ba3006ccec14f31b9ed304c7070cbbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89b131d5772e4d73afb485f5f583f145": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7121790a20449a6b101835e158d22de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80aa14d9f8b24239a8ba88dc5bda1ec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0fd6bdf80d04938bc1c9a335b9063b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "902bb413d3ea4833b25d7f14e216fde2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}