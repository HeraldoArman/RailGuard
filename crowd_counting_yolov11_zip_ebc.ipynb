{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HeraldoArman/RailGuard/blob/main/crowd_counting_yolov11_zip_ebc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Repo Git"
      ],
      "metadata": {
        "id": "6_zTgJ3NHvuV"
      },
      "id": "6_zTgJ3NHvuV"
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone own repo\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "github_username = 'Delta-K-rist'\n",
        "github_repo_name = 'kai-ai-model'\n",
        "\n",
        "github_token = userdata.get('GITHUB_TOKEN_DELTA')\n",
        "\n",
        "authenticated_github_url = f'https://{github_username}:{github_token}@github.com/{github_username}/{github_repo_name}.git'\n",
        "\n",
        "!git clone {authenticated_github_url}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTehAwcTHx0_",
        "outputId": "5e6739ef-2b18-41cd-ad50-12767d88f61c"
      },
      "id": "FTehAwcTHx0_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'kai-ai-model'...\n",
            "remote: Enumerating objects: 64, done.\u001b[K\n",
            "remote: Total 64 (delta 0), reused 0 (delta 0), pack-reused 64 (from 1)\u001b[K\n",
            "Receiving objects: 100% (64/64), 241.15 MiB | 28.25 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n",
            "Updating files: 100% (24/24), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Yiming-M/ZIP.git ZIP_Crowd_Counting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEtHShnyEeo_",
        "outputId": "d6f8a3f5-0f02-4e7b-f8d5-b0146f3304fe"
      },
      "id": "kEtHShnyEeo_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ZIP_Crowd_Counting'...\n",
            "remote: Enumerating objects: 126, done.\u001b[K\n",
            "remote: Counting objects: 100% (126/126), done.\u001b[K\n",
            "remote: Compressing objects: 100% (108/108), done.\u001b[K\n",
            "remote: Total 126 (delta 33), reused 96 (delta 17), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (126/126), 2.85 MiB | 7.89 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installations"
      ],
      "metadata": {
        "id": "jFgrx6ggsK2R"
      },
      "id": "jFgrx6ggsK2R"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q ultralytics opencv-python-headless"
      ],
      "metadata": {
        "id": "VRFekfdosOUF"
      },
      "id": "VRFekfdosOUF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q ultralytics \"lap>=0.5.12\""
      ],
      "metadata": {
        "id": "XELADI8hfv5R"
      },
      "id": "XELADI8hfv5R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Check Versions"
      ],
      "metadata": {
        "id": "q-7wRHPyZ013"
      },
      "id": "q-7wRHPyZ013"
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import torch\n",
        "import torchvision\n",
        "import timm\n",
        "import numpy\n",
        "import scipy\n",
        "import yaml  # PyYAML library\n",
        "import peft\n",
        "\n",
        "print(\"--- System Information ---\")\n",
        "print(f\"Python Version: {sys.version.split()[0]}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"PyTorch Version: {torch.__version__}\")\n",
        "else:\n",
        "    print(\"CUDA is not available. PyTorch is running on CPU.\")\n",
        "    print(f\"PyTorch Version: {torch.__version__}\")\n",
        "\n",
        "print(\"\\n--- Key Package Versions ---\")\n",
        "print(f\"Torchvision: {torchvision.__version__}\")\n",
        "print(f\"Timm: {timm.__version__}\")\n",
        "print(f\"PEFT: {peft.__version__}\")\n",
        "print(f\"NumPy: {numpy.__version__}\")\n",
        "print(f\"SciPy: {scipy.__version__}\")\n",
        "print(f\"PyYAML: {yaml.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBgDeGXOZrEE",
        "outputId": "0d388c1a-30e8-46e3-86c2-8e238d22954d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- System Information ---\n",
            "Python Version: 3.12.11\n",
            "CUDA Version: 12.6\n",
            "PyTorch Version: 2.8.0+cu126\n",
            "\n",
            "--- Key Package Versions ---\n",
            "Torchvision: 0.23.0+cu126\n",
            "Timm: 1.0.20\n",
            "PEFT: 0.17.1\n",
            "NumPy: 2.0.2\n",
            "SciPy: 1.16.2\n",
            "PyYAML: 6.0.3\n"
          ]
        }
      ],
      "id": "oBgDeGXOZrEE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Download Versions"
      ],
      "metadata": {
        "id": "zfBxD_hwZ3UF"
      },
      "id": "zfBxD_hwZ3UF"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Installing the specified list of packages...\")\n",
        "\n",
        "!pip install -q \\\n",
        "    einops \\\n",
        "    pyturbojpeg \\\n",
        "    tensorboardX \\\n",
        "    open_clip_torch\n",
        "\n",
        "print(\"‚úÖ All specified packages have been installed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkwPcIWfY1kH",
        "outputId": "c1ae6679-dd47-4e3c-f07a-328cc075e887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing the specified list of packages...\n",
            "‚úÖ All specified packages have been installed.\n"
          ]
        }
      ],
      "id": "TkwPcIWfY1kH"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Initialization (YOLOv11.m and EBC-ZIP)"
      ],
      "metadata": {
        "id": "TX5rNgr1ssLM"
      },
      "id": "TX5rNgr1ssLM"
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XpocO5oLz8q",
        "outputId": "adf9aead-3664-4b1d-ef14-7ba244fd96a4"
      },
      "id": "2XpocO5oLz8q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ZIP_Crowd_Counting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ZIP_Crowd_Counting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C50k_DwQEyOM",
        "outputId": "ae01c08c-d2ee-4caf-d219-c541e26398dc"
      },
      "id": "C50k_DwQEyOM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ZIP_Crowd_Counting/ZIP_Crowd_Counting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create the full directory path (your command was perfect for this)\n",
        "!mkdir -p /content/ZIP_Crowd_Counting/checkpoints/sha\n",
        "\n",
        "# 2. Download the file directly into that new directory\n",
        "# The -P flag specifies the output directory\n",
        "!wget https://github.com/Yiming-M/ZIP/releases/download/weights_sha/ebc_s.zip -P /content/ZIP_Crowd_Counting/checkpoints/sha\n",
        "\n",
        "# 3. Unzip the file in its destination directory\n",
        "# The -d flag specifies the destination directory for the unzipped files\n",
        "!unzip /content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s.zip -d /content/ZIP_Crowd_Counting/checkpoints/sha"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMyD6NZL_oLp",
        "outputId": "0db7a76a-6b8d-4d9b-8c26-90190eed72d7"
      },
      "id": "qMyD6NZL_oLp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-10-03 19:44:17--  https://github.com/Yiming-M/ZIP/releases/download/weights_sha/ebc_s.zip\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/1009736049/cfa45342-7cfe-4056-8b36-168b9e05fb78?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-10-03T20%3A44%3A47Z&rscd=attachment%3B+filename%3Debc_s.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-10-03T19%3A43%3A55Z&ske=2025-10-03T20%3A44%3A47Z&sks=b&skv=2018-11-09&sig=88tgv2UeqmtUPmt2nvkgcCLIrIjn6F7veU9W6CnvIQ0%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1OTUyNDI1NywibmJmIjoxNzU5NTIwNjU3LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.q2pfFauvIClODrvlUsJGChGliNFiEQ4wVTxSHCs2jgk&response-content-disposition=attachment%3B%20filename%3Debc_s.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-10-03 19:44:17--  https://release-assets.githubusercontent.com/github-production-release-asset/1009736049/cfa45342-7cfe-4056-8b36-168b9e05fb78?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-10-03T20%3A44%3A47Z&rscd=attachment%3B+filename%3Debc_s.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-10-03T19%3A43%3A55Z&ske=2025-10-03T20%3A44%3A47Z&sks=b&skv=2018-11-09&sig=88tgv2UeqmtUPmt2nvkgcCLIrIjn6F7veU9W6CnvIQ0%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc1OTUyNDI1NywibmJmIjoxNzU5NTIwNjU3LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.q2pfFauvIClODrvlUsJGChGliNFiEQ4wVTxSHCs2jgk&response-content-disposition=attachment%3B%20filename%3Debc_s.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 504535392 (481M) [application/octet-stream]\n",
            "Saving to: ‚Äò/content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s.zip.1‚Äô\n",
            "\n",
            "ebc_s.zip.1         100%[===================>] 481.16M  42.7MB/s    in 11s     \n",
            "\n",
            "2025-10-03 19:44:28 (45.5 MB/s) - ‚Äò/content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s.zip.1‚Äô saved [504535392/504535392]\n",
            "\n",
            "Archive:  /content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s.zip\n",
            "replace /content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s/best_mae.log? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace /content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s/best_mae.pth? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace /content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s/best_nae.log? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace /content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s/best_nae.pth? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace /content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s/best_rmse.log? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "n\n",
            "replace /content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s/best_rmse.pth? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "if '.' not in sys.path:\n",
        "    sys.path.append('.')\n",
        "\n",
        "import torch\n",
        "from ultralytics import YOLO\n",
        "from models import get_model\n",
        "\n",
        "def initialize_all_models(zip_checkpoint_path):\n",
        "    \"\"\"\n",
        "    Loads and initializes both the YOLOv8 and ZIP models.\n",
        "    \"\"\"\n",
        "    print(\"üß† Initializing all AI models...\")\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # --- Load YOLOv8 Model ---\n",
        "    yolo_model = YOLO('yolo11m.pt')\n",
        "    yolo_model.to(device)\n",
        "    print(f\"YOLOv11 model loaded on {device}.\")\n",
        "\n",
        "    # --- Load ZIP Model (using your reference code) ---\n",
        "    print(f\"Loading ZIP model from: {zip_checkpoint_path}\")\n",
        "    zip_model = get_model(zip_checkpoint_path)\n",
        "    zip_model = zip_model.to(device)\n",
        "    zip_model.eval() # Set model to evaluation mode\n",
        "    print(f\"ZIP model loaded on {device}.\")\n",
        "\n",
        "    # Return all models in a dictionary\n",
        "    models = {\n",
        "        'yolo': yolo_model,\n",
        "        'zip': zip_model,\n",
        "        'device': device # Store the device for later use\n",
        "    }\n",
        "    return models"
      ],
      "metadata": {
        "id": "7tZ1ZoZ7-Qb2"
      },
      "id": "7tZ1ZoZ7-Qb2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ZIP Setup"
      ],
      "metadata": {
        "id": "Wr-4KW-pSUo-"
      },
      "id": "Wr-4KW-pSUo-"
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision.transforms import ToTensor, Normalize, Compose\n",
        "import cv2\n",
        "\n",
        "# Define the transformation for the ZIP model once\n",
        "zip_transform = Compose([\n",
        "    ToTensor(),\n",
        "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "def get_count_from_density_map(frame, zip_model, device):\n",
        "    \"\"\"\n",
        "    Takes a single video frame, processes it with the ZIP model,\n",
        "    and returns both the estimated person count and the density map.\n",
        "    \"\"\"\n",
        "    # ... (Image conversion and transformation logic is the same)\n",
        "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    image_pil = Image.fromarray(image_rgb)\n",
        "    image_tensor = zip_transform(image_pil)\n",
        "    image_tensor = image_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predicted_density = zip_model(image_tensor)\n",
        "\n",
        "    predicted_count = predicted_density.sum().item()\n",
        "\n",
        "    # --- MODIFIED: Return the density map as well ---\n",
        "    return predicted_count, predicted_density"
      ],
      "metadata": {
        "id": "uaHgftP-B0fF"
      },
      "id": "uaHgftP-B0fF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define path to your ZIP model checkpoint ---\n",
        "# MAKE SURE YOU HAVE UPLOADED THIS FILE\n",
        "ZIP_CHECKPOINT = \"/content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s/best_mae.pth\"\n",
        "\n",
        "# 1. Load both models\n",
        "all_models = initialize_all_models(ZIP_CHECKPOINT)\n",
        "\n",
        "# 2. Create a dummy frame to test the density function\n",
        "# (A black image of size 640x480)\n",
        "import numpy as np\n",
        "dummy_frame = np.zeros((480, 640, 3), dtype=np.uint8)\n",
        "\n",
        "# 3. Run the test\n",
        "if all_models.get('zip'):\n",
        "    # --- MODIFIED: Unpack the tuple into two variables ---\n",
        "    test_count, test_density_map = get_count_from_density_map(dummy_frame, all_models['zip'], all_models['device'])\n",
        "\n",
        "    print(f\"\\n‚úÖ Test successful!\")\n",
        "    # This print statement will now work correctly\n",
        "    print(f\"Count from dummy frame using ZIP model: {test_count:.2f}\")\n",
        "else:\n",
        "    print(\"\\n‚ùå Test failed. ZIP model not loaded.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JyF3UbqMzoV",
        "outputId": "033fcb03-fda0-4ca5-883d-e6074c4ec18d"
      },
      "id": "8JyF3UbqMzoV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß† Initializing all AI models...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m.pt to 'yolo11m.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 38.8MB 211.5MB/s 0.2s\n",
            "YOLOv11 model loaded on cuda.\n",
            "Loading ZIP model from: /content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s/best_mae.pth\n",
            "ZIP model loaded on cuda.\n",
            "\n",
            "‚úÖ Test successful!\n",
            "Count from dummy frame using ZIP model: 0.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video_with_fusion(video_path, models, output_dir, polygon_coords):\n",
        "    \"\"\"\n",
        "    Final version:\n",
        "    - Calculation: Spatial Fusion (YOLO outside + ZIP inside).\n",
        "    - Visualization: Uses the automatic, rich yolo.plot() output blended\n",
        "      with the heatmap, avoiding all manual box drawing.\n",
        "    \"\"\"\n",
        "    print(f\"üìπ Starting Spatial Fusion processing for: {video_path}\")\n",
        "\n",
        "    # --- Video Setup (same as before) ---\n",
        "    cap = cv2.VideoCapture(video_path); frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)); frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)); fps = int(cap.get(cv2.CAP_PROP_FPS)); cap.release()\n",
        "    output_video_path = os.path.join(output_dir, \"output_video_spatial_fusion.mp4\"); fourcc = cv2.VideoWriter_fourcc(*'mp4v'); writer = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    yolo_model, zip_model, device = models['yolo'], models['zip'], models['device']\n",
        "    yolo_results_generator = yolo_model.track(source=video_path, persist=True, tracker=\"bytetrack.yaml\", classes=0, stream=True, verbose=False)\n",
        "\n",
        "    roi_polygon = np.array(polygon_coords, dtype=np.int32)\n",
        "    all_frames_data = []; inference_times = []\n",
        "\n",
        "    for frame_results in yolo_results_generator:\n",
        "        original_frame = frame_results.orig_img\n",
        "        inference_times.append(frame_results.speed['inference'])\n",
        "        yolo_boxes = frame_results.boxes.data.cpu().numpy()\n",
        "        yolo_count = len(yolo_boxes)\n",
        "\n",
        "        final_annotated_frame, final_count, avg_confidence, confidences = (None, 0, 0.0, [])\n",
        "\n",
        "        if yolo_count < 10:\n",
        "            # --- Case 1: VERY SPARSE (Unchanged) ---\n",
        "            final_count = yolo_count\n",
        "            final_annotated_frame = frame_results.plot(line_width=1, font_size=0.2)\n",
        "            cv2.polylines(final_annotated_frame, [roi_polygon], isClosed=True, color=(0, 255, 0), thickness=2)\n",
        "            cv2.putText(final_annotated_frame, \"MODE: Pure YOLO (Sparse)\", (30, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
        "            if frame_results.boxes.conf is not None:\n",
        "                confidences = frame_results.boxes.conf.tolist()\n",
        "                if confidences: avg_confidence = sum(confidences) / len(confidences)\n",
        "\n",
        "        else:\n",
        "            # --- Case 2: MEDIUM/DENSE - Spatial Fusion Calculation ---\n",
        "            zip_count_full, density_map = get_count_from_density_map(original_frame, zip_model, device)\n",
        "            outside_yolo_count = 0\n",
        "            for box in yolo_boxes:\n",
        "                box_center = (int((box[0] + box[2]) / 2), int((box[1] + box[3]) / 2))\n",
        "                if cv2.pointPolygonTest(roi_polygon, box_center, False) < 0:\n",
        "                    outside_yolo_count += 1\n",
        "            density_map_cpu = density_map.squeeze().cpu().numpy()\n",
        "            scale_x, scale_y = density_map_cpu.shape[1] / frame_width, density_map_cpu.shape[0] / frame_height\n",
        "            scaled_polygon = (roi_polygon * [scale_x, scale_y]).astype(np.int32)\n",
        "            mask = np.zeros(density_map_cpu.shape, dtype=np.uint8); cv2.fillPoly(mask, [scaled_polygon], 255)\n",
        "            inside_zip_count = (density_map_cpu * (mask / 255.0)).sum()\n",
        "            final_count = outside_yolo_count + round(inside_zip_count)\n",
        "\n",
        "            # --- MODIFIED: Reverted to the .plot() visualization ---\n",
        "            # 1. Get the default, rich annotation from YOLO's .plot()\n",
        "            yolo_annotated_frame = frame_results.plot(line_width=1, font_size=0.5)\n",
        "\n",
        "            # 2. Generate the full-frame heatmap\n",
        "            norm_map = cv2.normalize(density_map_cpu, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n",
        "            heatmap = cv2.applyColorMap(norm_map, cv2.COLORMAP_JET)\n",
        "            heatmap = cv2.resize(heatmap, (frame_width, frame_height))\n",
        "\n",
        "            # 3. Blend the full YOLO annotations with the heatmap\n",
        "            blended_frame = cv2.addWeighted(yolo_annotated_frame, 0.6, heatmap, 0.4, 0)\n",
        "\n",
        "            # 4. Draw the polygon on top\n",
        "            cv2.polylines(blended_frame, [roi_polygon], isClosed=True, color=(0, 255, 0), thickness=2)\n",
        "\n",
        "            final_annotated_frame = blended_frame\n",
        "            # We no longer need the manual drawing text, just the final mode\n",
        "            cv2.putText(final_annotated_frame, f\"MODE: Spatial Fusion\", (30, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "            cv2.putText(final_annotated_frame, f\"  - Outside (YOLO): {outside_yolo_count}\", (30, 110), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "            cv2.putText(final_annotated_frame, f\"  - Inside (ZIP-EBC): {round(inside_zip_count)}\", (30, 140), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
        "\n",
        "            avg_confidence = np.mean(yolo_boxes[:, 5]) if len(yolo_boxes) > 0 else 0\n",
        "            confidences = yolo_boxes[:, 5].tolist()\n",
        "\n",
        "        # Add the final count text to every frame\n",
        "        cv2.putText(final_annotated_frame, f\"Final Count: {final_count}\", (30, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
        "        writer.write(final_annotated_frame)\n",
        "\n",
        "        all_frames_data.append({ \"count\": final_count, \"avg_confidence\": avg_confidence, \"annotated_frame\": final_annotated_frame, \"original_frame\": original_frame, \"confidences\": confidences })\n",
        "\n",
        "    # ... (Phase 2 analysis remains unchanged) ...\n",
        "    writer.release(); print(\"‚úÖ Data collection complete.\")\n",
        "    print(\"üß† Starting Phase 2: Finding the best frame...\");\n",
        "    if not all_frames_data: return [], None, None, [], []\n",
        "    absolute_max_count = max(frame['count'] for frame in all_frames_data)\n",
        "    candidate_frames = [f for f in all_frames_data if f['count'] >= absolute_max_count * 0.95]\n",
        "    best_frame = max(candidate_frames, key=lambda x: x['avg_confidence']) if candidate_frames else max(all_frames_data, key=lambda x: x['count'])\n",
        "    print(f\"üèÜ Best frame selected: Count={best_frame['count']}, Confidence={best_frame['avg_confidence']:.2f}\")\n",
        "    final_annotated_snapshot, final_original_snapshot = best_frame['annotated_frame'].copy(), best_frame['original_frame'].copy()\n",
        "    final_confidences_at_max = best_frame['confidences']\n",
        "    frame_by_frame_counts = [frame['count'] for frame in all_frames_data]\n",
        "\n",
        "    return frame_by_frame_counts, final_annotated_snapshot, final_original_snapshot, inference_times, final_confidences_at_max"
      ],
      "metadata": {
        "id": "1QkI0WcENmbE"
      },
      "id": "1QkI0WcENmbE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "GAuTZ9X8YUWn"
      },
      "id": "GAuTZ9X8YUWn"
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "def preprocess_video(input_path, output_path, target_fps=15, target_width=1280):\n",
        "    \"\"\"\n",
        "    Creates an optimized version of a video for faster model processing.\n",
        "\n",
        "    Args:\n",
        "        input_path (str): Path to the original video file.\n",
        "        output_path (str): Path to save the new, preprocessed video.\n",
        "        target_fps (int): The desired frames per second.\n",
        "        target_width (int): The desired frame width. Height is scaled automatically.\n",
        "\n",
        "    Returns:\n",
        "        str: The path to the newly created video file.\n",
        "    \"\"\"\n",
        "    print(f\"üîß Starting preprocessing for {input_path}...\")\n",
        "\n",
        "    # 1. Open the original video\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video.\")\n",
        "        return None\n",
        "\n",
        "    # 2. Get original video properties\n",
        "    original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    original_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    original_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # 3. Calculate resampling and resizing parameters\n",
        "    skip_interval = max(1, round(original_fps / target_fps))\n",
        "    aspect_ratio = original_height / original_width\n",
        "    target_height = int(target_width * aspect_ratio)\n",
        "\n",
        "    print(f\"Original: {original_width}x{original_height} @ {original_fps:.2f} FPS\")\n",
        "    print(f\"Target:   {target_width}x{target_height} @ {target_fps} FPS (keeping 1 in every {skip_interval} frames)\")\n",
        "\n",
        "    # 4. Initialize the Video Writer for the new video\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    writer = cv2.VideoWriter(output_path, fourcc, target_fps, (target_width, target_height))\n",
        "\n",
        "    # 5. Loop, Resample, Resize, and Write\n",
        "    frame_number = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break # End of video\n",
        "\n",
        "        # Only process a frame if it's at the correct interval\n",
        "        if frame_number % skip_interval == 0:\n",
        "            # Resize the frame\n",
        "            resized_frame = cv2.resize(frame, (target_width, target_height))\n",
        "            # Write the resized frame to the new video\n",
        "            writer.write(resized_frame)\n",
        "\n",
        "        frame_number += 1\n",
        "\n",
        "    # 6. Finalize and clean up\n",
        "    cap.release()\n",
        "    writer.release()\n",
        "\n",
        "    print(f\"‚úÖ Preprocessing complete. Optimized video saved to: {output_path}\")\n",
        "    return output_path"
      ],
      "metadata": {
        "id": "2pBx87PIYWP0"
      },
      "id": "2pBx87PIYWP0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save"
      ],
      "metadata": {
        "id": "2RUugP0_Svex"
      },
      "id": "2RUugP0_Svex"
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# STEP 2.2 (UPDATED): THE SAVE RESULTS FUNCTION\n",
        "# -----------------------------------------------------------------------------\n",
        "import os\n",
        "import json\n",
        "import cv2\n",
        "\n",
        "def save_analysis_summary(output_dir, counts, annotated_snapshot, original_snapshot, inference_times, video_basename, confidences_at_max):\n",
        "    \"\"\"\n",
        "    Calculates final metrics and saves the output files, including both snapshot versions.\n",
        "    \"\"\"\n",
        "    print(f\"üíæ Saving analysis to: {output_dir}\")\n",
        "\n",
        "    if video_basename == 'd_1':\n",
        "        gerbong_id = 'gerbong_3'\n",
        "    elif video_basename == 'm_1':\n",
        "        gerbong_id = 'gerbong_2'\n",
        "    elif video_basename == 's_2':\n",
        "        gerbong_id = 'gerbong_1'\n",
        "    else:\n",
        "        gerbong_id = 'gerbong_unknown' # Default case\n",
        "\n",
        "    avg_confidence = 0\n",
        "    if confidences_at_max:\n",
        "        avg_confidence = sum(confidences_at_max) / len(confidences_at_max)\n",
        "\n",
        "    # ... (all the calculation logic remains the same) ...\n",
        "    human_count = int(max(counts)) if counts else 0\n",
        "    MEDIUM_THRESHOLD = 13\n",
        "    DANGEROUS_THRESHOLD = 27\n",
        "    crowdness_level = \"Low Density\"\n",
        "    if human_count >= DANGEROUS_THRESHOLD:\n",
        "        crowdness_level = \"High Density\"\n",
        "    elif human_count >= MEDIUM_THRESHOLD:\n",
        "        crowdness_level = \"Medium Density\"\n",
        "    total_inference_seconds = sum(inference_times) / 1000.0\n",
        "    avg_inference_ms = sum(inference_times) / len(inference_times) if inference_times else 0\n",
        "\n",
        "    # --- MODIFIED: Save both snapshot images with new names ---\n",
        "    if annotated_snapshot is not None:\n",
        "        # text = f\"Human Count: {human_count} ({crowdness_level})\"\n",
        "        # cv2.putText(annotated_snapshot, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "        snapshot_path = os.path.join(output_dir, 'snapshot_annotated.jpg')\n",
        "        cv2.imwrite(snapshot_path, annotated_snapshot)\n",
        "        print(f\"üì∏ Annotated snapshot saved to {snapshot_path}\")\n",
        "\n",
        "    if original_snapshot is not None:\n",
        "        snapshot_path_orig = os.path.join(output_dir, 'snapshot_original.jpg')\n",
        "        cv2.imwrite(snapshot_path_orig, original_snapshot)\n",
        "        print(f\"üì∏ Original snapshot saved to {snapshot_path_orig}\")\n",
        "\n",
        "    # ... (JSON saving logic remains the same) ...\n",
        "    results_data = {\n",
        "        'gerbong_id': gerbong_id,\n",
        "        'max_human_count': human_count,\n",
        "        'confidence_score': round(avg_confidence, 2),\n",
        "        'crowdness_level': crowdness_level,\n",
        "        'performance': {\n",
        "            'total_inference_seconds': round(total_inference_seconds, 2),\n",
        "            'average_inference_ms': round(avg_inference_ms, 2),\n",
        "            'average_fps': round(1000 / avg_inference_ms, 1) if avg_inference_ms > 0 else 'inf'\n",
        "        },\n",
        "        'frame_count_data': counts\n",
        "    }\n",
        "    json_path = os.path.join(output_dir, 'results.json')\n",
        "    with open(json_path, 'w') as f:\n",
        "        json.dump(results_data, f, indent=4)\n",
        "    print(f\"üìÑ JSON summary saved to {json_path}\")"
      ],
      "metadata": {
        "id": "tSYHBpx9vF0o"
      },
      "id": "tSYHBpx9vF0o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run"
      ],
      "metadata": {
        "id": "e9PJixYLTHis"
      },
      "id": "e9PJixYLTHis"
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# FINAL MAIN EXECUTION BLOCK - BATCH PROCESSING WITH AI FUSION\n",
        "# -----------------------------------------------------------------------------\n",
        "import os\n",
        "import glob\n",
        "from IPython.display import display, Video\n",
        "import json\n",
        "\n",
        "# --- 1. Define Base Paths ---\n",
        "INPUT_FOLDER = '/content/kai-ai-model/YOLO/dataset/test/demo'\n",
        "BASE_OUTPUT_FOLDER = '/content/kai-ai-model/YOLO/dataset/test result'\n",
        "ZIP_CHECKPOINT = \"/content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s/best_mae.pth\"\n",
        "\n",
        "\n",
        "# The polygon coordinates you provided\n",
        "ROI_POLYGON = [[371, 247], [868, 250], [837, 443], [391, 433]]\n",
        "os.makedirs(BASE_OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "# --- 2. Find all video files ---\n",
        "video_files = sorted(list(set(\n",
        "    glob.glob(os.path.join(INPUT_FOLDER, '*.mp4')) + glob.glob(os.path.join(INPUT_FOLDER, '*.MP4')) +\n",
        "    glob.glob(os.path.join(INPUT_FOLDER, '*.mov')) + glob.glob(os.path.join(INPUT_FOLDER, '*.MOV'))\n",
        ")))\n",
        "print(f\"Found {len(video_files)} videos to process.\")\n",
        "\n",
        "# --- 3. Initialize all models ONCE ---\n",
        "print(\"\\n--- INITIALIZING ALL MODELS (ONCE) ---\")\n",
        "all_models = initialize_all_models(ZIP_CHECKPOINT)\n",
        "\n",
        "# --- 4. Loop through each video and run the full pipeline ---\n",
        "for original_video_path in video_files:\n",
        "    print(f\"\\n{'='*50}\\nüé¨ PROCESSING VIDEO: {os.path.basename(original_video_path)}\\n{'='*50}\")\n",
        "\n",
        "    video_basename = os.path.splitext(os.path.basename(original_video_path))[0]\n",
        "    final_output_dir = os.path.join(BASE_OUTPUT_FOLDER, video_basename)\n",
        "    os.makedirs(final_output_dir, exist_ok=True)\n",
        "\n",
        "    preprocessed_video_path = os.path.join(final_output_dir, 'preprocessed.mp4')\n",
        "    actual_video_to_process = preprocess_video(original_video_path, preprocessed_video_path)\n",
        "\n",
        "    if actual_video_to_process:\n",
        "        counts, annotated_snap, original_snap, times, confs = process_video_with_fusion(\n",
        "            actual_video_to_process,\n",
        "            all_models,\n",
        "            final_output_dir,\n",
        "            polygon_coords=ROI_POLYGON # Pass the polygon to the function\n",
        "        )\n",
        "\n",
        "        save_analysis_summary(\n",
        "            final_output_dir, counts, annotated_snap,\n",
        "            original_snap, times, video_basename, confs\n",
        "        )\n",
        "        print(f\"‚úÖ SUCCESSFULLY PROCESSED: {os.path.basename(original_video_path)}\")\n",
        "    else:\n",
        "        print(f\"‚ùå FAILED to preprocess: {os.path.basename(original_video_path)}\")\n",
        "\n",
        "print(f\"\\n\\n{'='*50}\\nüéâ BATCH PROCESSING COMPLETE! üéâ\\n{'='*50}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEa65UvrNqo9",
        "outputId": "fde9e925-469e-4e56-b55a-62851328a150"
      },
      "id": "aEa65UvrNqo9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3 videos to process.\n",
            "\n",
            "--- INITIALIZING ALL MODELS (ONCE) ---\n",
            "üß† Initializing all AI models...\n",
            "YOLOv11 model loaded on cuda.\n",
            "Loading ZIP model from: /content/ZIP_Crowd_Counting/checkpoints/sha/ebc_s/best_mae.pth\n",
            "ZIP model loaded on cuda.\n",
            "\n",
            "==================================================\n",
            "üé¨ PROCESSING VIDEO: d_1.mp4\n",
            "==================================================\n",
            "üîß Starting preprocessing for /content/kai-ai-model/YOLO/dataset/test/demo/d_1.mp4...\n",
            "Original: 1280x720 @ 30.00 FPS\n",
            "Target:   1280x720 @ 15 FPS (keeping 1 in every 2 frames)\n",
            "‚úÖ Preprocessing complete. Optimized video saved to: /content/kai-ai-model/YOLO/dataset/test result/d_1/preprocessed.mp4\n",
            "üìπ Starting Spatial Fusion processing for: /content/kai-ai-model/YOLO/dataset/test result/d_1/preprocessed.mp4\n",
            "‚úÖ Data collection complete.\n",
            "üß† Starting Phase 2: Finding the best frame...\n",
            "üèÜ Best frame selected: Count=27, Confidence=0.61\n",
            "üíæ Saving analysis to: /content/kai-ai-model/YOLO/dataset/test result/d_1\n",
            "üì∏ Annotated snapshot saved to /content/kai-ai-model/YOLO/dataset/test result/d_1/snapshot_annotated.jpg\n",
            "üì∏ Original snapshot saved to /content/kai-ai-model/YOLO/dataset/test result/d_1/snapshot_original.jpg\n",
            "üìÑ JSON summary saved to /content/kai-ai-model/YOLO/dataset/test result/d_1/results.json\n",
            "‚úÖ SUCCESSFULLY PROCESSED: d_1.mp4\n",
            "\n",
            "==================================================\n",
            "üé¨ PROCESSING VIDEO: m_1.MOV\n",
            "==================================================\n",
            "üîß Starting preprocessing for /content/kai-ai-model/YOLO/dataset/test/demo/m_1.MOV...\n",
            "Original: 1920x1080 @ 29.97 FPS\n",
            "Target:   1280x720 @ 15 FPS (keeping 1 in every 2 frames)\n",
            "‚úÖ Preprocessing complete. Optimized video saved to: /content/kai-ai-model/YOLO/dataset/test result/m_1/preprocessed.mp4\n",
            "üìπ Starting Spatial Fusion processing for: /content/kai-ai-model/YOLO/dataset/test result/m_1/preprocessed.mp4\n",
            "‚úÖ Data collection complete.\n",
            "üß† Starting Phase 2: Finding the best frame...\n",
            "üèÜ Best frame selected: Count=24, Confidence=0.54\n",
            "üíæ Saving analysis to: /content/kai-ai-model/YOLO/dataset/test result/m_1\n",
            "üì∏ Annotated snapshot saved to /content/kai-ai-model/YOLO/dataset/test result/m_1/snapshot_annotated.jpg\n",
            "üì∏ Original snapshot saved to /content/kai-ai-model/YOLO/dataset/test result/m_1/snapshot_original.jpg\n",
            "üìÑ JSON summary saved to /content/kai-ai-model/YOLO/dataset/test result/m_1/results.json\n",
            "‚úÖ SUCCESSFULLY PROCESSED: m_1.MOV\n",
            "\n",
            "==================================================\n",
            "üé¨ PROCESSING VIDEO: s_2.MOV\n",
            "==================================================\n",
            "üîß Starting preprocessing for /content/kai-ai-model/YOLO/dataset/test/demo/s_2.MOV...\n",
            "Original: 1920x1080 @ 29.97 FPS\n",
            "Target:   1280x720 @ 15 FPS (keeping 1 in every 2 frames)\n",
            "‚úÖ Preprocessing complete. Optimized video saved to: /content/kai-ai-model/YOLO/dataset/test result/s_2/preprocessed.mp4\n",
            "üìπ Starting Spatial Fusion processing for: /content/kai-ai-model/YOLO/dataset/test result/s_2/preprocessed.mp4\n",
            "‚úÖ Data collection complete.\n",
            "üß† Starting Phase 2: Finding the best frame...\n",
            "üèÜ Best frame selected: Count=14, Confidence=0.52\n",
            "üíæ Saving analysis to: /content/kai-ai-model/YOLO/dataset/test result/s_2\n",
            "üì∏ Annotated snapshot saved to /content/kai-ai-model/YOLO/dataset/test result/s_2/snapshot_annotated.jpg\n",
            "üì∏ Original snapshot saved to /content/kai-ai-model/YOLO/dataset/test result/s_2/snapshot_original.jpg\n",
            "üìÑ JSON summary saved to /content/kai-ai-model/YOLO/dataset/test result/s_2/results.json\n",
            "‚úÖ SUCCESSFULLY PROCESSED: s_2.MOV\n",
            "\n",
            "\n",
            "==================================================\n",
            "üéâ BATCH PROCESSING COMPLETE! üéâ\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The '-r' flag means 'recursive' to include all subfolders\n",
        "# !zip -r /content/results.zip '/content/kai-ai-model/YOLO/dataset/test result'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaqx5OWpo5iL",
        "outputId": "61d737d3-b0d5-41f1-fad6-fbec4b19772e"
      },
      "id": "zaqx5OWpo5iL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "updating: content/kai-ai-model/YOLO/dataset/test result/ (stored 0%)\n",
            "updating: content/kai-ai-model/YOLO/dataset/test result/m_1/ (stored 0%)\n",
            "updating: content/kai-ai-model/YOLO/dataset/test result/m_1/snapshot_original.jpg (deflated 0%)\n",
            "updating: content/kai-ai-model/YOLO/dataset/test result/m_1/results.json (deflated 89%)\n",
            "updating: content/kai-ai-model/YOLO/dataset/test result/m_1/preprocessed.mp4 (deflated 0%)\n",
            "updating: content/kai-ai-model/YOLO/dataset/test result/m_1/output_video_spatial_fusion.mp4 (deflated 1%)\n",
            "updating: content/kai-ai-model/YOLO/dataset/test result/m_1/snapshot_annotated.jpg (deflated 0%)\n",
            "updating: content/kai-ai-model/YOLO/dataset/test result/s_2/ (stored 0%)\n",
            "updating: content/kai-ai-model/YOLO/dataset/test result/s_2/snapshot_original.jpg (deflated 0%)\n",
            "updating: content/kai-ai-model/YOLO/dataset/test result/s_2/results.json (deflated 88%)\n",
            "updating: content/kai-ai-model/YOLO/dataset/test result/s_2/preprocessed.mp4 (deflated 0%)\n",
            "updating: content/kai-ai-model/YOLO/dataset/test result/s_2/output_video_spatial_fusion.mp4 (deflated 1%)\n",
            "updating: content/kai-ai-model/YOLO/dataset/test result/s_2/snapshot_annotated.jpg (deflated 0%)\n",
            "updating: content/kai-ai-model/YOLO/dataset/test result/nothing.txt (stored 0%)\n",
            "updating: content/kai-ai-model/YOLO/dataset/test result/d_1/ (stored 0%)\n",
            "updating: content/kai-ai-model/YOLO/dataset/test result/d_1/snapshot_original.jpg (deflated 0%)\n",
            "updating: content/kai-ai-model/YOLO/dataset/test result/d_1/results.json (deflated 88%)\n",
            "updating: content/kai-ai-model/YOLO/dataset/test result/d_1/preprocessed.mp4 (deflated 0%)\n",
            "updating: content/kai-ai-model/YOLO/dataset/test result/d_1/output_video_spatial_fusion.mp4 (deflated 2%)\n",
            "updating: content/kai-ai-model/YOLO/dataset/test result/d_1/snapshot_annotated.jpg (deflated 1%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn pyngrok nest_asyncio"
      ],
      "metadata": {
        "id": "TPd6F6yK3O-S",
        "outputId": "95dd4682-4090-44d6-bfa5-b66ce412f394",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "TPd6F6yK3O-S",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.118.0)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.37.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.4.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.48.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fastapi) (2.11.9)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (4.15.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.3.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.49.0,>=0.40.0->fastapi) (4.11.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.49.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.49.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading pyngrok-7.4.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.4.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "q-7wRHPyZ013"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}